{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus1 = ['Tiger hunts to eat', \"wulf and Lion hunt to feed their family\", 'Leopord hunts to eat first and feed his family']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus used is \n",
      "['and' 'eat' 'family' 'feed' 'first' 'his' 'hunt' 'hunts' 'leopord' 'lion'\n",
      " 'their' 'tiger' 'to' 'wulf'] \n",
      "resulting as \n",
      "[[0 1 0 0 0 0 0 1 0 0 0 1 1 0]\n",
      " [1 0 1 1 0 0 1 0 0 1 1 0 1 1]\n",
      " [1 1 1 1 1 1 0 1 1 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "c1 = vectorizer.fit_transform(corpus1)\n",
    "\n",
    "print(f'Corpus used is \\n{vectorizer.get_feature_names_out()} \\nresulting as \\n{c1.toarray()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordered Vocabulary: ['tiger', 'hunts', 'to', 'eat', 'wulf', 'and', 'lion', 'hunt', 'feed', 'their', 'family', 'leopord', 'first', 'his']\n",
      "\n",
      "Vectorized Output:\n",
      " [[1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 1 1 1 1 1 1 0 0 0]\n",
      " [0 1 1 1 0 1 0 0 1 0 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Extract words in order of appearance\n",
    "ordered_vocab = []\n",
    "for sentence in corpus1:\n",
    "    for word in sentence.split():  # Splitting each sentence into words\n",
    "        if word.lower() not in ordered_vocab:\n",
    "            ordered_vocab.append(word.lower())  # Keeping lowercase for consistency\n",
    "\n",
    "# Create Vectorizer with a fixed vocabulary\n",
    "vectorizer = CountVectorizer(vocabulary=ordered_vocab)\n",
    "\n",
    "X = vectorizer.transform(corpus1)\n",
    "\n",
    "print(\"Ordered Vocabulary:\", ordered_vocab)\n",
    "print(\"\\nVectorized Output:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fight\n",
      "defenc\n",
      "run\n",
      "miss\n",
      "deal\n",
      "have\n",
      "ador\n",
      "best\n",
      "wa\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "print(ps.stem('Fighting'))\n",
    "print(ps.stem('Defence'))\n",
    "print(ps.stem('running'))\n",
    "print(ps.stem('missed'))\n",
    "print(ps.stem('dealed'))\n",
    "print(ps.stem('having'))\n",
    "print(ps.stem('Adorable'))\n",
    "print(ps.stem('Best'))\n",
    "print(ps.stem('Was'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abhis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mouse'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "lem = WordNetLemmatizer()\n",
    "lem.lemmatize('mice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "going\n",
      "Best\n",
      "best\n",
      "be\n",
      "wa \n",
      "\n",
      "foot\n",
      "car\n"
     ]
    }
   ],
   "source": [
    "print(lem.lemmatize('going', pos=wordnet.VERB))\n",
    "print(lem.lemmatize('going'))\n",
    "print(lem.lemmatize('Best'))\n",
    "print(lem.lemmatize('best', pos=wordnet.ADJ))\n",
    "print(lem.lemmatize('was', pos='v'))\n",
    "print(lem.lemmatize('was'),'\\n')\n",
    "\n",
    "print(lem.lemmatize('feet'))\n",
    "print(lem.lemmatize('cars'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "good\n",
      "goose\n",
      "be\n",
      "study\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(lem.lemmatize(\"running\", pos=\"v\"))\n",
    "print(lem.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lem.lemmatize(\"geese\", pos=\"n\"))\n",
    "print(lem.lemmatize(\"was\", pos=\"v\"))\n",
    "print(lem.lemmatize(\"studies\", pos=\"n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    \n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\abhis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"I will surely fulfill my trusted destiny with my enormous blessings and learning\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('surely', 'RB'),\n",
       " ('fulfill', 'VB'),\n",
       " ('my', 'PRP$'),\n",
       " ('trusted', 'JJ'),\n",
       " ('destiny', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('enormous', 'JJ'),\n",
       " ('blessings', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('learning', 'VBG')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_and_tags = nltk.pos_tag(sent)\n",
    "words_and_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will surely fulfill my trusted destiny with my enormous blessing and learn "
     ]
    }
   ],
   "source": [
    "for word, tag in words_and_tags:\n",
    "    lem_res = lem.lemmatize(word, pos=get_wordnet_pos(tag))\n",
    "    print(lem_res, end=' ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will surely fulfill my trusted destiny with my enormous blessing and learning "
     ]
    }
   ],
   "source": [
    "for token in sent:\n",
    "    lems = lem.lemmatize(token)\n",
    "    print(lems, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The children are playing in the garden.\",\n",
    "    \"She is running faster than her friends.\",\n",
    "    \"He was reading an interesting book.\",\n",
    "    \"They have been watching movies all day.\",\n",
    "    \"The cats are chasing the mice around the house.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Original sentence: The children are playing in the garden.\n",
      "Lemmatized sentence: The child be play in the garden. \n",
      "\n",
      "2 Original sentence: She is running faster than her friends.\n",
      "Lemmatized sentence: She be run faster than her friends. \n",
      "\n",
      "3 Original sentence: He was reading an interesting book.\n",
      "Lemmatized sentence: He be read an interesting book. \n",
      "\n",
      "4 Original sentence: They have been watching movies all day.\n",
      "Lemmatized sentence: They have be watch movie all day. \n",
      "\n",
      "5 Original sentence: The cats are chasing the mice around the house.\n",
      "Lemmatized sentence: The cat be chase the mouse around the house. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for one_sent in sentences:\n",
    "    words_and_tags = nltk.pos_tag(one_sent.split())\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in words_and_tags:\n",
    "        lemmatized_sentence.append(lem.lemmatize(word, pos=get_wordnet_pos(tag)))\n",
    "    lemmatized_sentence = ' '.join(lemmatized_sentence)\n",
    "\n",
    "    print(sentences.index(one_sent)+1, \"Original sentence:\", one_sent)\n",
    "    print(\"Lemmatized sentence:\", lemmatized_sentence, '\\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
